{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python script below is converting the contents of the played_at field from Unix Timestamp to normal Date and Time\n",
    "Then fianlly takeing the converted data into a csv file which will be overwritten every time this script is ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|played_at          |\n",
      "+-------------------+\n",
      "|2025-03-29 01:30:46|\n",
      "|2025-03-29 01:32:49|\n",
      "|2025-03-29 01:35:48|\n",
      "|2025-03-29 01:37:49|\n",
      "|2025-03-29 01:42:10|\n",
      "|2025-03-29 01:45:10|\n",
      "|2025-03-29 01:48:15|\n",
      "|2025-03-29 01:51:23|\n",
      "|2025-03-29 01:54:55|\n",
      "|2025-03-29 01:57:53|\n",
      "|2025-03-29 02:00:28|\n",
      "|2025-03-29 02:02:31|\n",
      "|2025-03-29 02:05:23|\n",
      "|2025-03-29 02:07:52|\n",
      "|2025-03-29 02:10:10|\n",
      "|2025-03-29 02:12:11|\n",
      "|2025-03-29 02:16:11|\n",
      "|2025-03-29 02:19:38|\n",
      "|2025-03-29 02:22:25|\n",
      "|2025-03-29 02:25:22|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SpotifyDataNormalization\").getOrCreate()\n",
    "\n",
    "# Read data into a dataframe\n",
    "df = spark.read.option(\"multiline\", \"true\").json(\"all_recent_tracks.json\")\n",
    "\n",
    "# Convert timestamp to normal date and time \n",
    "df = df.withColumn(\"played_at\", F.from_unixtime(F.col(\"played_at\") / 1000, \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Creating A CSV file of the transformed data\n",
    "df.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"clean_tracks.csv\")\n",
    "\n",
    "# Show the transformed column 'played_at'\n",
    "df.select(\"played_at\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script although commented plays a crucial role in some projects with it's intedned purpose being to move the needed CSV file out of the folder pyspark created after the creation of the CSV file containing the transformed data.\n",
    "Why is it commented ? some may ask , well the name of the file changes each time the script is re-ran so the path of the file keeps changing and given the fact the data is aimed to keep growing, this is not feasible for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\IFEOMA\\AppData\\Local\\Temp\\ipykernel_9276\\441510881.py:1: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  \"\"\" import shutil\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' import shutil\\n# Move the part file to desired location\\npart_file = \"clean_tracks.csv\\\\part-00000-e767b6d4-f389-44a7-91b0-dec958759f38-c000.csv\"\\ndestination = \"clean_recent_tracks.csv\"\\nshutil.move(part_file,destination) '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import shutil\n",
    "# Move the part file to desired location\n",
    "part_file = \"clean_tracks.csv\\part-00000-e767b6d4-f389-44a7-91b0-dec958759f38-c000.csv\"\n",
    "destination = \"clean_recent_tracks.csv\"\n",
    "shutil.move(part_file,destination) \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below trims down the id for a track into just the numbers , yes the spotify:track: may be part of the id .\n",
    "I used this as a way to strech my skills in spliting entries and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+--------------------+----+----------+--------------------+\n",
      "|    artist_name|duration|                  id|link|popularity|          track_name|\n",
      "+---------------+--------+--------------------+----+----------+--------------------+\n",
      "|  Claire Leslie|  180006|4HR5BN6hc4AmcPO1N...|NULL|        45|                24/7|\n",
      "|     Grace Marr|  155494|5BH8UixV8wu3FR5xJ...|NULL|         2|              Belong|\n",
      "|    Cade Kellam|  242886|7LcJx95sWXpqCMLxQ...|NULL|        44|             Blessed|\n",
      "|           OAKS|  196130|2TUW3yDwsfNcBOtk8...|NULL|        27|               Clean|\n",
      "|Gabriel Eziashi|  817850|2SHWUh366VIpgpDHx...|NULL|        36|Contemporary Prai...|\n",
      "|      ONE HOUSE|  304015|4Cr2oltVAPC6rAUMT...|NULL|        29|    Down In My Heart|\n",
      "|Gabriel Eziashi|  178512|3zfPYqEqCoO6ud6PM...|NULL|        42|           Dry Bones|\n",
      "|  DÃ¢mares Gomes|  198688|1V9AYsVkGvDL0XrkY...|NULL|        37|         Even Better|\n",
      "|    Cade Kellam|  294186|4sZLEFie6r4WxCjCU...|NULL|        25|            Faithful|\n",
      "|  Claire Leslie|  170201|4PqpoY1wcn4GsM4gK...|NULL|        37|Falling At The Th...|\n",
      "| Samantha Ebert|  226000|0KIv0Lho9vsPCj8Sa...|NULL|        54|             Flowers|\n",
      "|  Alabaster Co.|  332991|0FYUGkkaI0OpnHVeH...|NULL|        16|For the Beauty of...|\n",
      "|          G.E.S|  130841|5JD1bFe82TNbO4TaJ...|NULL|        40|    Giving It to You|\n",
      "|   Paul Tomisin|  423093|4ZWMGSggxHp87KTvR...|NULL|        45|         Grace Story|\n",
      "|         sxxnt.|   88520|3MuYxyhq6m5OLG6NC...|NULL|        37|Great Is Thy Fait...|\n",
      "|        DKG KIE|  164181|6qLrRZYhtWKkanyYO...|NULL|        45|           HOLY HUH?|\n",
      "|       Don Moen|  263720|0O0XO2TXrVSojmmYP...|NULL|        52|  How Great Thou Art|\n",
      "|         sxxnt.|  228026|5LF2W7bxWbF4QiaL6...|NULL|        37|     I Surrender All|\n",
      "|    Sarah Juers|  226063|4V2rRHpWP0cH2GwsS...|NULL|        45|I can always find...|\n",
      "|    Cade Kellam|  280000|4HBcbRSYjIcUA1I7g...|NULL|        14|               Idols|\n",
      "+---------------+--------+--------------------+----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.option(\"multiline\", \"true\").json(\"all_top_tracks.json\")\n",
    "\n",
    "# Split by \":\" and get just the id which is the last part\n",
    "df2 = df2.withColumn(\"id\", F.split(\"id\", \":\").getItem(2))\n",
    "\n",
    "df2.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"clean_top_racks.csv\")\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
